{"type": "test", "ok": true, "event_id": "c88365f8-3bf5-4dc0-b920-daeb7382e600", "ts": 1766728822.482235}
{"type": "qa", "question": "From the Adam paper, what is it about?", "mode": "qa", "abstained": true, "answer": "I cannot find evidence in the provided papers to answer that.", "retrieved": [{"chunk_id": "63b426d11cff", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 1, "section_name": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION", "score": 0.48067379301204366, "text": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba∗\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally efﬁcient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the inﬁnity norm.\n1\nINTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many ﬁelds of science and\nengineering. Many problems in these ﬁelds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the"}, {"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.47343888802977235, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4709174005177695, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "0b073b4a92f1", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.46905663934876307, "text": "Published as a conference paper at ICLR 2015\n4\nCONVERGENCE ANALYSIS\nWe analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(θ), f2(θ),..., fT (θ). At\neach time t, our goal is to predict the parameter θt and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(θt)\nand the best ﬁxed point parameter ft(θ∗) from a feasible set X for all the previous steps. Concretely,\nthe regret is deﬁned as:\nR(T) =\nT\nX\nt=1\n[ft(θt) −ft(θ∗)]\n(5)\nwhere θ∗= arg minθ∈X\nPT\nt=1 ft(θ). We show Adam has O(\n√\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some deﬁnitions simplify our notation, where gt ≜∇ft(θt) and gt,i\nas the ith element. We deﬁne g1:t,i ∈Rt as a vector that contains the ith dimension of the gradients\nover all iterations till t, g1:t,i = [g1,i, g2,i, · · · , gt,i]. Also,"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.46857611558892476, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "65da86a11275", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 14, "section_name": "TbvT,i+", "score": 0.4474697494393698, "text": "Published as a conference paper at ICLR 2015\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}, and β1, β2 ∈[0, 1) satisfy\nβ2\n1\n√β2 < 1. Let αt =\nα\n√\nt\nand β1,t = β1λt−1, λ ∈(0, 1). Adam achieves the following guarantee, for all T ≥1.\nR(T) ≤\nD2\n2α(1 −β1)\nd\nX\ni=1\np\nTbvT,i+\nα(β1 + 1)G∞\n(1 −β1)√1 −β2(1 −γ)2\nd\nX\ni=1\n∥g1:T,i∥2+\nd\nX\ni=1\nD2\n∞G∞\n√1 −β2\n2α(1 −β1)(1 −λ)2\nProof. Using Lemma 10.2, we have,\nft(θt) −ft(θ∗) ≤gT\nt (θt −θ∗) =\nd\nX\ni=1\ngt,i(θt,i −θ∗\n,i)\nFrom the update rules presented in algorithm 1,\nθt+1 = θt −αt bmt/\np\nbvt\n= θt −\nαt\n1 −βt\n1\n\u0012 β1,t\n√bvt\nmt−1 + (1 −"}], "cited_chunks": [], "event_id": "50c00b6e-5d7f-4f72-a3d0-3de48e9eadab", "ts": 1766728998.69276}
{"type": "qa", "question": "From the Adam paper, what is it about?", "mode": "qa", "abstained": true, "answer": "I cannot find evidence in the provided papers to answer that.", "retrieved": [{"chunk_id": "63b426d11cff", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 1, "section_name": "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION", "score": 0.48067379301204366, "text": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba∗\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally efﬁcient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the inﬁnity norm.\n1\nINTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many ﬁelds of science and\nengineering. Many problems in these ﬁelds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the"}, {"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.47343888802977235, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4709174005177695, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "0b073b4a92f1", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.46905663934876307, "text": "Published as a conference paper at ICLR 2015\n4\nCONVERGENCE ANALYSIS\nWe analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(θ), f2(θ),..., fT (θ). At\neach time t, our goal is to predict the parameter θt and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(θt)\nand the best ﬁxed point parameter ft(θ∗) from a feasible set X for all the previous steps. Concretely,\nthe regret is deﬁned as:\nR(T) =\nT\nX\nt=1\n[ft(θt) −ft(θ∗)]\n(5)\nwhere θ∗= arg minθ∈X\nPT\nt=1 ft(θ). We show Adam has O(\n√\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some deﬁnitions simplify our notation, where gt ≜∇ft(θt) and gt,i\nas the ith element. We deﬁne g1:t,i ∈Rt as a vector that contains the ith dimension of the gradients\nover all iterations till t, g1:t,i = [g1,i, g2,i, · · · , gt,i]. Also,"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.46857611558892476, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "65da86a11275", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 14, "section_name": "TbvT,i+", "score": 0.4474697494393698, "text": "Published as a conference paper at ICLR 2015\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}, and β1, β2 ∈[0, 1) satisfy\nβ2\n1\n√β2 < 1. Let αt =\nα\n√\nt\nand β1,t = β1λt−1, λ ∈(0, 1). Adam achieves the following guarantee, for all T ≥1.\nR(T) ≤\nD2\n2α(1 −β1)\nd\nX\ni=1\np\nTbvT,i+\nα(β1 + 1)G∞\n(1 −β1)√1 −β2(1 −γ)2\nd\nX\ni=1\n∥g1:T,i∥2+\nd\nX\ni=1\nD2\n∞G∞\n√1 −β2\n2α(1 −β1)(1 −λ)2\nProof. Using Lemma 10.2, we have,\nft(θt) −ft(θ∗) ≤gT\nt (θt −θ∗) =\nd\nX\ni=1\ngt,i(θt,i −θ∗\n,i)\nFrom the update rules presented in algorithm 1,\nθt+1 = θt −αt bmt/\np\nbvt\n= θt −\nαt\n1 −βt\n1\n\u0012 β1,t\n√bvt\nmt−1 + (1 −"}], "cited_chunks": [], "event_id": "6491d5f1-1df1-4d3b-a807-fb3e08853de4", "ts": 1766729001.273103}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": true, "answer": "I cannot find evidence in the provided papers to answer that.", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622706379676996, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45896595702074555, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4537468426501287, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4463386389935485, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.44598380762286755, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.43904017324186523, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [], "event_id": "fee05456-c6c0-4532-a427-0e115e362ebd", "ts": 1766729518.543253}
{"type": "qa", "question": "\"What is Adam's bias correction technique and why is it needed?\"", "mode": "qa", "abstained": true, "answer": "I cannot find evidence in the provided papers to answer that.", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.5281324909554894, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.5046628903115778, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.4870052520533958, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "8860c4dffe6d", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4774372063046284, "text": "second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad:\nAn algorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its\nbasic version updates parameters as θt+1 = θt −α · gt/\nqPt\ni=1 g2\nt . Note that if we choose β2 to be\ninﬁnitesimally close to 1 from below, then limβ2→1 bvt = t−1 · Pt\ni=1 g2\nt . AdaGrad corresponds to a\nversion of Adam with β1 = 0, inﬁnitesimal (1 −β2) and a replacement of α by an annealed version\nαt = α · t−1/2, namely θt −α · t−1/2 · bmt/\np\nlimβ2→1 bvt = θt −α · t−1/2 · gt/\nq\nt−1 · Pt\ni=1 g2\nt =\nθt −α · gt/\nqPt\ni=1 g2\nt . Note that this direct correspondence between Adam and Adagrad does\nnot hold when removing the bias-correction terms; without bias correction, like in RMSProp, a β2\ninﬁnitesimally close to 1 would lead to"}, {"chunk_id": "f066fb88a5e3", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.47662962505467377, "text": "Published as a conference paper at ICLR 2015\nβ1=0\nβ1=0.9\nβ2=0.99\nβ2=0.999\nβ2=0.9999\nβ2=0.99\nβ2=0.999\nβ2=0.9999\n(a) after 10 epochs\n(b) after 100 epochs\nlog10(α)\nLoss\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto-\nEncoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize α (x-axes) and hyper-\nparameters β1 and β2.\n6.4\nEXPERIMENT: BIAS-CORRECTION TERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.\nDiscussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele-\nman & Hinton, 2012) with momentum. We vary the β1 and β2 when training a variational auto-\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0."}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4763191488564342, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}], "cited_chunks": [], "event_id": "ea348c10-ad04-42df-9fee-6a7dc0862cb8", "ts": 1766729642.339267}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": false, "answer": "Adam is an optimization algorithm designed for stochastic optimization, particularly effective in high-dimensional machine learning problems. It combines the advantages of two other popular methods: AdaGrad and RMSProp. Adam maintains a running average of both the first moment (mean) and the second moment (uncentered variance) of the gradients, which allows it to adaptively adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients [be4c37dd5c8f p.2].\n\nThe update rule for Adam is defined as follows: at each timestep \\( t \\), it computes the biased first moment estimate \\( m_t \\) and the biased second moment estimate \\( v_t \\). These estimates are then bias-corrected to account for their initialization at zero. The parameters are updated using the formula:\n\n\\[\n\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{b_m}{\\sqrt{b_v} + \\epsilon}\n\\]\n\nwhere \\( b_m \\) and \\( b_v \\) are the bias-corrected estimates of the first and second moments, respectively, and \\( \\alpha \\) is the learning rate [be4c37dd5c8f p.2].\n\nAdam's adaptive learning rates help it perform well across various models and datasets, consistently outperforming other optimization methods [be4c37dd5c8f p.2]. The algorithm is particularly robust to sparse gradients, making it suitable for problems where the gradients can be highly variable [6e4854f2bbb4 p.5].", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622706379676996, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45896595702074555, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4537468426501287, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4463386389935485, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.44598380762286755, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.43904017324186523, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}], "event_id": "9dbdb633-0f3a-47cd-8a2a-518767aa4043", "ts": 1766729783.4600399}
{"type": "qa", "question": "\"What is Adam's bias correction technique and why is it needed?\"", "mode": "qa", "abstained": false, "answer": "Adam's bias correction technique involves adjusting the first and second moment estimates of the gradients to counteract the initialization bias that occurs when the moving averages are still influenced by their initial values. Specifically, the bias-corrected first moment estimate is computed as \\( bmt = mt/(1 - \\beta_t^1) \\) and the bias-corrected second raw moment estimate as \\( bvt = vt/(1 - \\beta_t^2) \\) [be4c37dd5c8f p.2]. \n\nThis technique is particularly important when using values of \\( \\beta_2 \\) close to 1, which are required for robustness to sparse gradients. Without bias correction, the algorithm can experience large stepsizes that may lead to divergence, especially in the early stages of training [6e4854f2bbb4 p.8; 8860c4dffe6d p.5]. The empirical results demonstrate that the bias correction term significantly improves stability and performance during optimization, particularly when gradients become sparser as training progresses [6e4854f2bbb4 p.8].", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.5280768524015714, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.5046085503775838, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.4869472420338988, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "8860c4dffe6d", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4774100345363027, "text": "second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad:\nAn algorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its\nbasic version updates parameters as θt+1 = θt −α · gt/\nqPt\ni=1 g2\nt . Note that if we choose β2 to be\ninﬁnitesimally close to 1 from below, then limβ2→1 bvt = t−1 · Pt\ni=1 g2\nt . AdaGrad corresponds to a\nversion of Adam with β1 = 0, inﬁnitesimal (1 −β2) and a replacement of α by an annealed version\nαt = α · t−1/2, namely θt −α · t−1/2 · bmt/\np\nlimβ2→1 bvt = θt −α · t−1/2 · gt/\nq\nt−1 · Pt\ni=1 g2\nt =\nθt −α · gt/\nqPt\ni=1 g2\nt . Note that this direct correspondence between Adam and Adagrad does\nnot hold when removing the bias-correction terms; without bias correction, like in RMSProp, a β2\ninﬁnitesimally close to 1 would lead to"}, {"chunk_id": "f066fb88a5e3", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.47661462239494556, "text": "Published as a conference paper at ICLR 2015\nβ1=0\nβ1=0.9\nβ2=0.99\nβ2=0.999\nβ2=0.9999\nβ2=0.99\nβ2=0.999\nβ2=0.9999\n(a) after 10 epochs\n(b) after 100 epochs\nlog10(α)\nLoss\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto-\nEncoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize α (x-axes) and hyper-\nparameters β1 and β2.\n6.4\nEXPERIMENT: BIAS-CORRECTION TERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.\nDiscussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele-\nman & Hinton, 2012) with momentum. We vary the β1 and β2 when training a variational auto-\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0."}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.47626360244932087, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}], "cited_chunks": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}], "event_id": "dbdc5b12-6bae-4af4-b249-940683968428", "ts": 1766729955.0613499}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": false, "answer": "Adam is an optimization algorithm designed for stochastic optimization, particularly effective in high-dimensional machine learning problems. It combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. Adam maintains a running average of both the first moment (mean) and the second moment (uncentered variance) of the gradients, which allows it to adaptively adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients [be4c37dd5c8f p.2].\n\nThe update rule for Adam involves computing bias-corrected estimates of these moments to counteract the initialization bias that can occur, especially when the decay rates (β1 and β2) are close to 1. The parameters are updated using the formula: \n\n\\[\n\\theta_t = \\theta_{t-1} - \\alpha_t \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n\\]\n\nwhere \\(m_t\\) is the bias-corrected first moment estimate, \\(v_t\\) is the bias-corrected second moment estimate, and \\(\\alpha_t\\) is the effective learning rate [be4c37dd5c8f p.2]. \n\nAdam's convergence properties are theoretically analyzed, showing that it achieves a regret bound of \\(O(1/\\sqrt{T})\\), which indicates that the average regret converges to zero as the number of iterations increases [07f48f55df67 p.4]. This makes Adam particularly suitable for problems with noisy gradients and sparse data [6aa0e1a4f897 p.5]. \n\nOverall, Adam is recognized for its versatility and efficiency in handling large-scale optimization tasks, outperforming other methods across various models and datasets [be4c37dd5c8f p.2].", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622706379676996, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45896595702074555, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4537468426501287, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4463386389935485, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.44598380762286755, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.43904017324186523, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "event_id": "97586f78-5409-45bb-8c4c-177c6602d88a", "ts": 1766730001.714764}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": false, "answer": "Adam is an optimization algorithm designed for stochastic optimization, particularly effective in high-dimensional machine learning problems. It combines the advantages of two other popular methods: AdaGrad and RMSProp. Adam maintains a running average of both the first moment (mean) and the second moment (uncentered variance) of the gradients, which allows it to adaptively adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients [be4c37dd5c8f p.2].\n\nThe update rule for Adam is given by:\n\n$$\n\\theta_t = \\theta_{t-1} - \\alpha_t \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n$$\n\nwhere $\\alpha_t$ is the effective learning rate, $m_t$ is the bias-corrected first moment estimate, and $v_t$ is the bias-corrected second moment estimate [be4c37dd5c8f p.2]. The algorithm also includes bias correction terms to counteract the initialization bias that can occur, especially when the decay rates $\\beta_1$ and $\\beta_2$ are close to 1 [6e4854f2bbb4 p.8].\n\nAdam's convergence properties are theoretically analyzed, showing that it achieves a regret bound of $O(1/\\sqrt{T})$, which indicates that the average regret converges to zero as the number of iterations increases [07f48f55df67 p.4]. This makes Adam particularly suitable for problems where the objective function is noisy and the gradients are sparse [6aa0e1a4f897 p.5]. \n\nOverall, Adam is recognized for its versatility and efficiency in handling large-scale optimization tasks [be4c37dd5c8f p.2].", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622871459210415, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45897266186436303, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.45375157960420887, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.44633448301713435, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.4459970624173054, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4390445851271813, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}], "event_id": "8eb0e8d5-91ee-425b-ba24-320b804a82bf", "ts": 1766730148.721698}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": false, "answer": "Adam is an optimization algorithm designed for stochastic optimization, particularly effective in high-dimensional machine learning problems. It combines the advantages of two other popular methods: AdaGrad and RMSProp. Adam maintains a running average of both the first moment (mean) and the second moment (uncentered variance) of the gradients, which allows it to adaptively adjust the learning rate for each parameter based on the estimates of the first and second moments of the gradients [be4c37dd5c8f p.2].\n\nThe update rule for Adam is given by:\n\n1. Compute the biased first moment estimate: $m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$.\n2. Compute the biased second moment estimate: $v_t \\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$.\n3. Compute bias-corrected estimates: $b_m = \\frac{m_t}{1 - \\beta_1^t}$ and $b_v = \\frac{v_t}{1 - \\beta_2^t}$.\n4. Update the parameters: $\\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\frac{b_m}{\\sqrt{b_v} + \\epsilon}$ [be4c37dd5c8f p.2].\n\nThe algorithm's effectiveness is attributed to its careful choice of step sizes, which are adjusted based on the past gradients, allowing for more stable convergence, especially in scenarios with sparse gradients [78737aa50f4e p.2]. Adam has been shown to outperform other optimization methods across various models and datasets [be4c37dd5c8f p.2].", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622706379676996, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45896595702074555, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4537468426501287, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4463386389935485, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.44598380762286755, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.43904017324186523, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [{"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}], "event_id": "8cc2f9d5-ed03-4ec6-a9a7-803a8889e7c7", "ts": 1766730220.094229}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": false, "answer": "Adam is an optimization algorithm designed for stochastic optimization problems, particularly in machine learning. It combines the advantages of two other popular methods: AdaGrad and RMSProp. Adam stands for Adaptive Moment Estimation, and it utilizes adaptive learning rates for each parameter by maintaining a running average of both the first moment (mean) and the second moment (uncentered variance) of the gradients.\n\nThe update rule for Adam is defined as follows:\n\n1. Compute the gradients of the objective function with respect to the parameters.\n2. Update the biased first moment estimate: \n   $$ m_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$\n3. Update the biased second moment estimate:\n   $$ v_t \\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$\n4. Compute bias-corrected estimates:\n   $$ \\hat{m}_t \\leftarrow \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t \\leftarrow \\frac{v_t}{1 - \\beta_2^t} $$\n5. Update the parameters:\n   $$ \\theta_t \\leftarrow \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$\n\nHere, $\\alpha$ is the learning rate, $\\beta_1$ and $\\beta_2$ are the exponential decay rates for the moment estimates, and $\\epsilon$ is a small constant to prevent division by zero [be4c37dd5c8f p.2].\n\nAdam is particularly effective because it adapts the learning rate based on the first and second moments of the gradients, allowing for more stable and efficient convergence, especially in high-dimensional spaces [be4c37dd5c8f p.2]. The algorithm has been shown to outperform other optimization methods across various models and datasets [be4c37dd5c8f p.2]. \n\nIn terms of convergence, Adam achieves a regret bound of $O(1/\\sqrt{T})$, which indicates that the average regret converges to zero as the number of iterations increases [07f48f55df67 p.4]. This makes it a robust choice for training machine learning models, particularly when dealing with noisy or sparse data [6e4854f2bbb4 p.5].", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622706379676996, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45896595702074555, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4537468426501287, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4463386389935485, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.44598380762286755, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.43904017324186523, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [{"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}], "event_id": "fc2b2b18-045e-44ac-943e-bb7faa034d5e", "ts": 1766730324.013599}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": false, "answer": "Adam is an optimization algorithm designed for stochastic optimization, particularly effective in high-dimensional machine learning problems. It combines the advantages of two other popular methods: AdaGrad and RMSProp. Adam maintains two moving averages for each parameter: the first moment (mean) and the second moment (uncentered variance) of the gradients. The update rule for the parameters $\\theta$ at timestep $t$ is given by:\n\n$$\n\\theta_t = \\theta_{t-1} - \\alpha_t \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n$$\n\nwhere $m_t$ is the bias-corrected first moment estimate, $v_t$ is the bias-corrected second moment estimate, $\\alpha_t$ is the effective learning rate, and $\\epsilon$ is a small constant to prevent division by zero [be4c37dd5c8f p.2].\n\nThe algorithm adapts the learning rate based on the estimates of the first and second moments, allowing it to perform well across various datasets and models. The theoretical convergence of Adam has been established, showing that it achieves a regret bound of $O\\left(\\frac{1}{\\sqrt{T}}\\right)$, where $T$ is the number of iterations [07f48f55df67 p.4]. This makes Adam particularly suitable for problems with noisy gradients or sparse data [6e4854f2bbb4 p.5].", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622706379676996, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45896595702074555, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4537468426501287, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4463386389935485, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.44598380762286755, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.43904017324186523, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [{"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}], "event_id": "99f3524f-36b9-4884-9236-dd86e053191c", "ts": 1766730494.433934}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": false, "answer": "Adam is an optimization algorithm designed for stochastic optimization, particularly effective in high-dimensional machine learning problems. It combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. Adam maintains two moving averages for each parameter: the first moment (mean) and the second moment (uncentered variance) of the gradients.\n\nThe update rule for Adam is given by:\n\n$$\n\\theta_t = \\theta_{t-1} - \\alpha_t \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n$$\n\nwhere:\n- $\\theta_t$ are the parameters at time step $t$,\n- $\\alpha_t$ is the adaptive learning rate,\n- $m_t$ is the bias-corrected first moment estimate,\n- $v_t$ is the bias-corrected second moment estimate,\n- $\\epsilon$ is a small constant to prevent division by zero.\n\nThe parameters $m_t$ and $v_t$ are updated as follows:\n\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n$$\n\n$$\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n$$\n\nwhere $g_t$ is the gradient at time step $t$, and $\\beta_1$ and $\\beta_2$ are hyperparameters that control the decay rates of these moving averages [be4c37dd5c8f p.2].\n\nAdam's adaptive learning rate allows it to perform well across a variety of models and datasets, often outperforming other optimization methods [be4c37dd5c8f p.2]. The algorithm also includes a bias-correction mechanism to address the initialization bias that can occur, especially when $\\beta_2$ is close to 1 [78737aa50f4e p.2]. \n\nIn terms of convergence, Adam achieves a regret bound of $O\\left(\\frac{1}{\\sqrt{T}}\\right)$, indicating that as the number of iterations $T$ increases, the average regret approaches zero [07f48f55df67 p.4]. This makes Adam particularly suitable for online convex programming scenarios.", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622706379676996, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45896595702074555, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4537468426501287, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4463386389935485, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.44598380762286755, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.43904017324186523, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [{"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}], "event_id": "21fb5960-4ad4-4f19-a603-2f27b3fa5aac", "ts": 1766730659.0856671}
{"type": "qa", "question": "What is Adam and how does it work?", "mode": "qa", "abstained": false, "answer": "Adam is an optimization algorithm designed for stochastic optimization, particularly effective in high-dimensional machine learning problems. It combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. Adam maintains two moving averages for each parameter: the first moment (mean) and the second moment (uncentered variance) of the gradients. The update rule for the parameters $\\theta$ at timestep $t$ is given by:\n\n$$\n\\theta_t = \\theta_{t-1} - \\alpha_t \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\n$$\n\nwhere $m_t$ is the bias-corrected first moment estimate, $v_t$ is the bias-corrected second moment estimate, $\\alpha_t$ is the effective learning rate, and $\\epsilon$ is a small constant to prevent division by zero [be4c37dd5c8f p.2].\n\nThe algorithm adapts the learning rate based on the estimates of the first and second moments of the gradients, allowing for more efficient convergence. The effective step size taken in parameter space at timestep $t$ is:\n\n$$\n\\Delta_t = \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t}}\n$$\n\nThis adaptive nature helps in dealing with sparse gradients and varying learning rates across different parameters [78737aa50f4e p.2]. Adam has been shown to outperform other optimization methods across various models and datasets, making it a versatile choice for many machine learning tasks [be4c37dd5c8f p.2].", "retrieved": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4622706379676996, "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "07f48f55df67", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 4, "section_name": "CONVERGENCE ANALYSIS", "score": 0.45896595702074555, "text": "Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n√\nT),\nan improvement over O(\n√\ndT) for the non-adaptive method. Decaying β1,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical ﬁndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coefﬁcient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, ∥∇ft(θ)∥2 ≤G, ∥∇ft(θ)∥∞≤\nG∞for all θ ∈Rd and distance between any θt generated by Adam is bounded, ∥θn −θm∥2 ≤D,\n∥θm −θn∥∞≤D∞for any m, n ∈{1, ..., T}. Adam achieves the following guarantee, for all\nT ≥1.\nR(T)\nT\n= O( 1\n√\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 ∥g1:T,i∥2 ≤dG∞\n√\nT.\nThus,\nlimT →∞\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and Ada"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "score": 0.4537468426501287, "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}, {"chunk_id": "e75fed7bdbf9", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.4463386389935485, "text": "Published as a conference paper at ICLR 2015\nfrom ﬁrst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like\nAdaGrad’s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of ﬁrst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence,"}, {"chunk_id": "6e4854f2bbb4", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 8, "section_name": "Loss", "score": 0.44598380762286755, "text": "a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈[0, 0.9] and\nβ2 ∈[0.99, 0.999, 0.9999], and log10(α) ∈[−5, ..., −1]. Values of β2 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at ﬁrst few epochs of the training. The best results were achieved with small\nvalues of (1−β2) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speciﬁc patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p →∞, a surprisingly simple and stable algorithm\nemerges; see algorithm"}, {"chunk_id": "6aa0e1a4f897", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 5, "section_name": "RMSProp:", "score": 0.43904017324186523, "text": "theorat-\nical prediction from section 4. The logistic regression classiﬁes the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can efﬁciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n√\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the ﬁrst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5"}], "cited_chunks": [{"chunk_id": "be4c37dd5c8f", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "←β2 · vt−1 + (1 −β2) · g2\nt (Update biased second raw moment estimate)\nbmt ←mt/(1 −βt\n1) (Compute bias-corrected ﬁrst moment estimate)\nbvt ←vt/(1 −βt\n2) (Compute bias-corrected second raw moment estimate)\nθt ←θt−1 −α · bmt/(√bvt + ϵ) (Update parameters)\nend while\nreturn θt (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in-\nterested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With\nf1(θ), ..., , fT (θ) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = �"}, {"chunk_id": "78737aa50f4e", "source_file": "Adam A Method For Stochastic Optimization.pdf", "page_number": 2, "section_name": "and βt", "text": "improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\nαt = α ·\np\n1 −βt\n2/(1 −βt\n1) and θt ←θt−1 −αt · mt/(√vt + ˆϵ).\n2.1\nADAM’S UPDATE RULE\nAn important property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the\neffective step taken in parameter space at timestep t is ∆t = α · bmt/√bvt. The effective stepsize has\ntwo upper bounds: |∆t| ≤α · (1 −β1)/√1 −β2 in the case (1 −β1) > √1 −β2, and |∆t| ≤α\n2"}], "event_id": "5e01c68b-8a7b-4880-a39d-bdb31bc01de9", "ts": 1766730983.926857}
